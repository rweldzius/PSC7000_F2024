<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Multivariate Regression &amp; Logistic Regression</title>
    <meta charset="utf-8" />
    <meta name="author" content="Prof. Weldzius" />
    <script src="libs/header-attrs-2.27/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/lexis.css" type="text/css" />
    <link rel="stylesheet" href="css/lexis-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Multivariate Regression &amp; Logistic Regression
]
.subtitle[
## (Last Lecture!)
]
.author[
### Prof. Weldzius
]
.institute[
### Villanova University
]
.date[
### Slides Updated: 2024-11-23
]

---


&lt;style type="text/css"&gt;
.small .remark-code { /*Change made here*/
  font-size: 85% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 50% !important;
}
&lt;/style&gt;



# Part I: Linear Regression 

## Agenda

1. Recap of Movie Analysis

2. Multiple Regression

3. Categorical Predictors

---

# Recap of Movie Analysis


``` r
require(tidyverse)

mv &lt;- read_rds('https://github.com/rweldzius/PSC4175_F2024/raw/main/Data/mv.Rds')
```

--

- .blue[Theory]: the more a movie costs, the more it should make

--

  - If not, Hollywood would go out of business!
  
--

- `\(X\)`: budget

- `\(Y\)`: gross

---
# Area 51 vs. EEAAO

&lt;center&gt;&lt;img src="figs/Area51.jpg" width = "50%"&gt;&lt;/center&gt;
&lt;center&gt;&lt;img src="figs/eeaao.jpg" width = "50%"&gt;&lt;/center&gt;
---

# Step 1: Look


``` r
summary(mv %&gt;% select(gross,budget))
```

```
##      gross               budget         
##  Min.   :7.140e+02   Min.   :     5172  
##  1st Qu.:1.121e+07   1st Qu.: 16865322  
##  Median :5.178e+07   Median : 37212044  
##  Mean   :1.402e+08   Mean   : 57420173  
##  3rd Qu.:1.562e+08   3rd Qu.: 77844746  
##  Max.   :3.553e+09   Max.   :387367903  
##  NA's   :3668        NA's   :4482
```

---

# Step 1: Look


``` r
mv %&gt;%
  mutate(missing = ifelse(is.na(gross) | is.na(budget),1,0)) %&gt;%
  group_by(year) %&gt;%
  summarise(propMissing = mean(missing)) %&gt;%
* ggplot(aes(x = year,y = propMissing)) + geom_line()
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;


---

# Some quick wrangling


``` r
mv &lt;- mv %&gt;%
  drop_na(gross,budget)

mv %&gt;%
  select(gross,budget) %&gt;%
  glimpse()
```

```
## Rows: 3,179
## Columns: 2
## $ gross  &lt;dbl&gt; 73677478, 53278578, 723586629, 11490339, 62…
## $ budget &lt;dbl&gt; 93289619, 10883789, 160147179, 6996721, 139…
```

---

# Step 2: Univariate Viz


``` r
mv %&gt;%
  select(title,gross,budget) %&gt;%
  gather(metric,value,-title) %&gt;%
  ggplot(aes(x = value,color = metric)) + 
  geom_density()
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;

---

# More Wrangling?

--

- Univariate visualization higlights significant **skew** in both measures

  - Most movies don't cost a lot and don't make a lot

  - But there are a few blockbusters that pull the density way out

- Let's **wrangle** two new variables that take the log of these skewed measures

  - Logging transforms skewed measures to more "normal" measures


``` r
mv &lt;- mv %&gt;%
  mutate(gross_log = log(gross),
         budget_log = log(budget))
```

---

# Step 2: Univariate Viz


``` r
mv %&gt;%
  select(title,gross_log,budget_log) %&gt;%
  gather(metric,value,-title) %&gt;%
  ggplot(aes(x = value,color = metric)) + 
  geom_density()
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

---

# Step 3: Multivariate Viz


``` r
pClean &lt;- mv %&gt;%
  ggplot(aes(x = budget,y = gross)) + 
  geom_point() + 
  scale_x_log10(labels = scales::dollar) + 
  scale_y_log10(labels = scales::dollar) + 
  labs(title = "Movie Costs and Returns",
       x = "Costs (logged budget)",
       y = "Returns (logged gross)")
```

---

# Step 3: Multivariate Viz


``` r
pClean + geom_smooth(method = 'lm')
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-11-1.png" style="display: block; margin: auto;" /&gt;

---

# Step 4: Regression!


``` r
m &lt;- lm(gross_log ~ budget_log,data = mv)
summary(m)
```

```
## 
## Call:
## lm(formula = gross_log ~ budget_log, data = mv)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.2672 -0.6354  0.1648  0.7899  8.5599 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.26107    0.30953   4.074 4.73e-05 ***
## budget_log   0.96386    0.01786  53.971  &lt; 2e-16 ***
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.281 on 3177 degrees of freedom
## Multiple R-squared:  0.4783,	Adjusted R-squared:  0.4782 
## F-statistic:  2913 on 1 and 3177 DF,  p-value: &lt; 2.2e-16
```


---

# Step 5.1: Univariate Viz of Errors

- Errors `\(\varepsilon = Y - \hat{Y}\)`

  - In `R`, can also get them via `resid()` function


``` r
mv &lt;- mv %&gt;%
  mutate(errors_manual = gross_log - predict(m),
         errors_resid = resid(m))
```

---

# Step 5.1: Univariate Viz of Errors


``` r
mv %&gt;%
  ggplot(aes(x = errors_resid)) + 
  geom_histogram()
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;" /&gt;

---

# Step 5.2: Multivariate Viz of Errors


``` r
mv %&gt;%
  ggplot(aes(x = budget_log,y = errors_resid)) + 
  geom_point() + 
  geom_smooth()
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-15-1.png" style="display: block; margin: auto;" /&gt;

---

# Step 5.3: Cross Validated RMSE


``` r
set.seed(123)
rmseBudget &lt;- NULL
for(i in 1:100) {
  inds &lt;- sample(1:nrow(mv),size = round(nrow(mv)/2),replace = F)
  
  train &lt;- mv %&gt;% slice(inds)
  test &lt;- mv %&gt;% slice(-inds)
  
  mTrain &lt;- lm(gross_log ~ budget_log,train)
  
  test$preds &lt;- predict(mTrain,newdata = test)
  
  rmse &lt;- sqrt(mean((test$gross_log - test$preds)^2,na.rm=T))
  rmseBudget &lt;- c(rmseBudget,rmse)
}

mean(rmseBudget)
```

```
## [1] 1.279899
```

---


# Thinking like a .blue[scientist]

--

- Our previous model predicted `gross` as a function of `budget`

--

- .blue[Theoretically], is this sensible?

--
    
  1. Bigger budgets &amp;rarr; famous actors &amp;rarr; mass appeal &amp;rarr; more tickets
  
--

  2. Bigger budgets &amp;rarr; advertising money &amp;rarr; mass appeal &amp;rarr; more tickets
  
--

- But what if the movie is just...not good?


---

# Alternative .blue[Theory]

--

- Good movies make more money

--

  - .blue[Theory]: good movies &amp;rarr; recommendations &amp;rarr; more tickets
  
--

- Predict gross with .red[IMDB rating] (`score`)


``` r
pIMDB &lt;- mv %&gt;%
  ggplot(aes(x = score,y = gross)) + 
  geom_point() + 
  labs(title = "Movie gross as a function of public perception",
       x = "IMDB score",
       y = "Gross (logged)") + 
  scale_y_log10(label = scales::dollar) + 
  geom_smooth(method = 'lm',se = F)
```

---

# Alternative .red[Model]


``` r
pIMDB
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-18-1.png" style="display: block; margin: auto;" /&gt;

---

# Evaluating the Model

- Let's go straight to RMSE

  - We can have `R` calculate errors for us with `residuals()` command


``` r
m2 &lt;- lm(gross_log ~ score,mv)
error &lt;- residuals(m2)
(rmseScore &lt;- sqrt(mean(error^2)))
```

```
## [1] 1.753146
```

- Even worse!

---

# Multivariate Regression

--

- Recall that we can **model** our outcome with multiple **predictors**

`$$Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + \dots + \varepsilon$$`

--

- How much better can we predict `gross` with **BOTH** `budget` and `score`?


``` r
m3 &lt;- lm(gross_log ~ budget_log + score,mv)
error &lt;- residuals(m3)
(rmseBudgScore &lt;- sqrt(mean(error^2)))
```

```
## [1] 1.248817
```

---

# Comparing Models

--

- Which model best predicts movie revenues?


``` r
p &lt;- data.frame(budget = mean(rmseBudget),
           IMDB = rmseScore,
           combined = rmseBudgScore) %&gt;%
  gather(model,rmse) %&gt;%
  ggplot(aes(x = reorder(model,rmse),y = rmse)) + 
  geom_bar(stat = 'identity') + 
  labs(title = "Best Regression Model",
       subtitle = "Predicting a movie's revenue",
       y = "RMSE (lower is better)",
       x = "Model") + 
  coord_flip()
```

---

# Comparing Models

- Which model best predicts movie revenues?


``` r
p
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-22-1.png" style="display: block; margin: auto;" /&gt;


---

# Why RMSE?

--

- Want to understand how good / bad our model is

--

- Can use it to compare models

--

- Do we improve our model with `score`?

---

# Why RMSE?


``` r
set.seed(123)
bsRes &lt;- NULL
for(i in 1:100) {
  inds &lt;- sample(1:nrow(mv),size = round(nrow(mv)/2),replace = F)
  train &lt;- mv %&gt;% slice(inds)
  test &lt;- mv %&gt;% slice(-inds)
  
  mB &lt;- lm(gross_log ~ budget_log,train)
  mS &lt;- lm(gross_log ~ score,train)
  mC &lt;- lm(gross_log ~ budget_log + score,train)
  
  bsRes &lt;- test %&gt;%
    mutate(pB = predict(mB,newdata = test),
           pS = predict(mS,newdata = test),
           pC = predict(mC,newdata = test)) %&gt;%
    summarise(Budget = sqrt(mean((gross_log - pB)^2,na.rm=T)),
              Score = sqrt(mean((gross_log - pS)^2,na.rm=T)),
              Combined = sqrt(mean((gross_log - pC)^2,na.rm=T))) %&gt;%
    bind_rows(bsRes)
}
```

---

# Quick Aside: alternative `code`

- `sample_n()` and `anti_join()`


``` r
set.seed(123)
bsRes &lt;- NULL
for(i in 1:100) {
  train &lt;- mv %&gt;%
    sample_n(size = round(nrow(.)*.8),replace = F)
  test &lt;- mv %&gt;%
*   anti_join(train)

  mB &lt;- lm(gross_log ~ budget_log,train)
  mS &lt;- lm(gross_log ~ score,train)
  mC &lt;- lm(gross_log ~ budget_log + score,train)
  
  bsRes &lt;- test %&gt;%
    mutate(pB = predict(mB,newdata = test),
           pS = predict(mS,newdata = test),
           pC = predict(mC,newdata = test)) %&gt;%
    summarise(Budget = sqrt(mean((gross_log - pB)^2,na.rm=T)),
              Score = sqrt(mean((gross_log - pS)^2,na.rm=T)),
              Combined = sqrt(mean((gross_log - pC)^2,na.rm=T))) %&gt;%
    bind_rows(bsRes) 
}
```

---

# Why RMSE?


``` r
bsRes %&gt;%
  summarise_all(mean,na.rm=T)
```

```
## # A tibble: 1 × 3
##   Budget Score Combined
##    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
## 1   1.29  1.76     1.26
```

---

# Visualizing


``` r
bsRes %&gt;%
  gather(model,rmse) %&gt;%
  ggplot(aes(x = rmse,y = reorder(model,rmse))) + 
  geom_violin()
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-26-1.png" style="display: block; margin: auto;" /&gt;


---

# Categorical Data

--

- Thus far, only using continuous variables

--

- But we can do regression with categorical data too!

--

- The Bechdel Test: 3 questions of a movie

--

  1. Does it have two women in it?
  2. Who talk to each other?
  3. About something other than a man?
  

``` r
mv %&gt;%
  count(bechdel_score)
```

```
## # A tibble: 5 × 2
##   bechdel_score     n
##           &lt;dbl&gt; &lt;int&gt;
## 1             0   141
## 2             1   526
## 3             2   206
## 4             3  1185
## 5            NA  1121
```

---

# Research Question

--

- .blue[Do movies that pass the Bechdel Test make more money?]

--

  - .blue[Theory:] Women are ~50% of the population. Movies that pass the test are more appealing to women.
  
--

  - .blue[Hypothesis:] Movies that pass the test make more money.
  
--

- .red[Wrangling:] Let's turn the `bechdel_score` variable into a binary


``` r
mv &lt;- mv %&gt;%
  mutate(bechdel_bin = ifelse(bechdel_score == 3,1,0)) %&gt;%
  mutate(bechdel_factor=recode_factor(bechdel_bin,
                                      `1`="Pass",
                                      `0`="Fail",
                                      ))
```

---

# Regression

--

- We can add the binary factor to our regression


``` r
summary(lm(gross_log ~ bechdel_factor,mv))
```

```
## 
## Call:
## lm(formula = gross_log ~ bechdel_factor, data = mv)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.8817 -0.7918  0.2253  1.0831  3.8225 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)        18.16844    0.04835 375.794   &lt;2e-16 ***
## bechdel_factorFail  0.15969    0.07423   2.151   0.0316 *  
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.664 on 2056 degrees of freedom
##   (1121 observations deleted due to missingness)
## Multiple R-squared:  0.002246,	Adjusted R-squared:  0.001761 
## F-statistic: 4.628 on 1 and 2056 DF,  p-value: 0.03157
```

---

# Regression

- Coefficient is positive

--

- What is the interpretation?

--

  - Movies that fail make more money...
  
--

  - ...than what?
  
--

  - Movies that pass the Bechdel Test
  
--

- Categorical variables are **always interpreted in relation to the hold-out category**!

---

# Regression

- Movies that fail the test make more money!?

--

- **REMEMBER**: Correlation `\(\neq\)` causation

--

  - What might explain this pattern?
  
--

  - Budgets in a sexist Hollywood!
  
--

  - Movies that fail the test get larger budgets
  
--

  - Budgets are positively associated with gross
  
--

- So we want to "control" for budget by adding it to our regression


``` r
mBechCtrl &lt;- lm(gross_log ~ budget_log + bechdel_factor,mv)
```

---

# Regression


``` r
summary(mBechCtrl)
```

```
## 
## Call:
## lm(formula = gross_log ~ budget_log + bechdel_factor, data = mv)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.6325 -0.5305  0.1287  0.6792  7.9370 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)         2.30814    0.34497   6.691 2.85e-11 ***
## budget_log          0.92089    0.01993  46.199  &lt; 2e-16 ***
## bechdel_factorFail -0.18795    0.05254  -3.577 0.000356 ***
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.166 on 2055 degrees of freedom
##   (1121 observations deleted due to missingness)
## Multiple R-squared:  0.5106,	Adjusted R-squared:  0.5101 
## F-statistic:  1072 on 2 and 2055 DF,  p-value: &lt; 2.2e-16
```

---

# Regression

- Our hypothesis is supported!

--

- What about non-binary categorical variables?


``` r
mv %&gt;%
  count(rating)
```

```
## # A tibble: 9 × 2
##   rating        n
##   &lt;chr&gt;     &lt;int&gt;
## 1 G            54
## 2 NC-17         6
## 3 Not Rated    34
## 4 PG          434
## 5 PG-13      1249
## 6 R          1388
## 7 TV-MA         2
## 8 Unrated       7
## 9 &lt;NA&gt;          5
```

---

# Categorical

- Let's first remove rarely-occurring ratings


``` r
mvAnalysis &lt;- mv %&gt;%
  filter(!rating %in% c('Approved','TV-14','TV-MA','TV-PG','X'))
```

---

# Categorical


``` r
summary(lm(gross_log ~ rating,mvAnalysis))
```

```
## 
## Call:
## lm(formula = gross_log ~ rating, data = mvAnalysis)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.6749 -0.8189  0.1630  1.1082  5.2339 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      19.1818     0.2177  88.113  &lt; 2e-16 ***
## ratingNC-17      -2.4483     0.6884  -3.556 0.000381 ***
## ratingNot Rated  -4.4322     0.3502 -12.655  &lt; 2e-16 ***
## ratingPG         -0.3905     0.2308  -1.692 0.090784 .  
## ratingPG-13      -0.7633     0.2224  -3.433 0.000605 ***
## ratingR          -1.9123     0.2219  -8.618  &lt; 2e-16 ***
## ratingUnrated    -4.6564     0.6426  -7.246 5.38e-13 ***
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.6 on 3165 degrees of freedom
##   (5 observations deleted due to missingness)
## Multiple R-squared:  0.1772,	Adjusted R-squared:  0.1756 
## F-statistic: 113.6 on 6 and 3165 DF,  p-value: &lt; 2.2e-16
```

---

# Categorical

- Everything makes less money than the hold-out category!

--

  - "G"-rated movies are powered by children
  
--

- What if we wanted to compare to a different reference category?


``` r
mvAnalysis &lt;- mvAnalysis %&gt;%
  mutate(rating = factor(rating,
                         levels = c('R','PG-13','PG','G','Not Rated')))

mRating2 &lt;- lm(gross_log ~ rating,mvAnalysis)
```

---

# Categorical


``` r
summary(mRating2)
```

```
## 
## Call:
## lm(formula = gross_log ~ rating, data = mvAnalysis)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.6749 -0.8184  0.1610  1.1082  5.2339 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     17.26952    0.04296 402.005   &lt;2e-16 ***
## ratingPG-13      1.14905    0.06242  18.408   &lt;2e-16 ***
## ratingPG         1.52178    0.08802  17.289   &lt;2e-16 ***
## ratingG          1.91231    0.22199   8.614   &lt;2e-16 ***
## ratingNot Rated -2.51988    0.27782  -9.070   &lt;2e-16 ***
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.6 on 3154 degrees of freedom
##   (18 observations deleted due to missingness)
## Multiple R-squared:  0.1699,	Adjusted R-squared:  0.1689 
## F-statistic: 161.4 on 4 and 3154 DF,  p-value: &lt; 2.2e-16
```

---

# Cross Validation

- This is why `sample_n()` is useful


``` r
set.seed(123)
rmseRes_rating &lt;- NULL
for(i in 1:100) {
  train &lt;- mvAnalysis %&gt;%
    group_by(rating) %&gt;%
    sample_n(size = round(n()*.8),replace = F)
  test &lt;- mvAnalysis %&gt;% anti_join(train)
  
  m &lt;- lm(gross_log ~ rating,train)
  rmseRes_rating &lt;- test %&gt;%
    mutate(preds = predict(m,newdata = test)) %&gt;%
    summarise(rmse = sqrt(mean((gross_log - preds)^2,na.rm=T))) %&gt;%
    bind_rows(rmseRes_rating)
}
*rmseRes_rating %&gt;% summarise(rmse = mean(rmse))
```

```
## # A tibble: 1 × 1
##    rmse
##   &lt;dbl&gt;
## 1  1.60
```

---

# Break Time! 

&lt;center&gt;&lt;img src="figs/break3.jpg" height=440px width=640px&gt;&lt;/center&gt;


---

# Part II: Logistic Regression 

## Agenda

1. Classification

2. Fortnite gaming (i.e., Prof's desperate attempt to be relevant)


``` r
require(tidyverse)
fn &lt;- read_rds('https://github.com/rweldzius/PSC4175_F2024/raw/main/Data/fn_cleaned_final.rds')
```


---

# Definitions

- *Classification:* predicting the **class** of given data points via **predictive modeling**

--

  - *Class*: AKA targets, labels, or **categories**
  
--

  - *Predictive Modeling*: Approximate mapping function `\(f: X \rightarrow Y\)`
  
--

  - `\(X\)`: predictor variables
  
  - `\(Y\)`: outcome variable
  
  - `\(f\)`: ??
  
---

# Mapping Functions

- We have already used mapping functions!

--

- Linear Regression

--

  - `\(f\)`: `\(Y = \alpha + \beta X + \varepsilon\)`
  
--

- Underlying idea: `\(X\)` contain information about `\(Y\)`

---

# It is in the `\(Y\)`

- If `\(Y\)` is continuous, we use OLS (ordinary least squares) regression

--

- If `\(Y\)` is **binary**, we use "logistic" regression (AKA "logit")

--

  - As always, this is a **deep** area of study for those interested
  
--

- Today, using OLS for binary `\(Y\)`

--

  - Next few classes: replacing OLS regression with logit
  

---

# Fortnite

&lt;center&gt;&lt;img src="https://cdn2.unrealengine.com/blade-2560x1440-2560x1440-d4e556fb8166.jpg" width="80%"&gt;&lt;/center&gt;

---

# Fortnite

- Goal is to win (i.e., be the last player alive)

- Professional e-sports teams want to maximize this probability

- .blue[Research Question]: How can we increase the number of victories?

--

- **NB**: we are moving out of the **.blue[Research]** camp now, and into the **.red[Prediction]** world

--

  - We don't care so much about *why* a relationship exists, we just want to get accurate predictions

--

  - Theory can still help us, but want to start with the data to get our thinking started

---

# The Data


``` r
glimpse(fn)
```

```
## Rows: 957
## Columns: 24
## $ placed               &lt;dbl&gt; 17, 41, 36, 28, 3, 15, 9, 29,…
## $ mental_state         &lt;chr&gt; "sober", "sober", "high", "hi…
## $ eliminations         &lt;dbl&gt; 2, 0, 3, 1, 3, 0, 2, 3, 4, 1,…
## $ assists              &lt;dbl&gt; 0, 2, 0, 4, 2, 1, 2, 2, 0, 2,…
## $ revives              &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,…
## $ accuracy             &lt;dbl&gt; 0.19371429, 0.32400265, 0.336…
## $ hits                 &lt;dbl&gt; 10, 17, 38, 22, 49, 4, 43, 14…
## $ head_shots           &lt;dbl&gt; 1, 0, 0, 3, 18, 3, 2, 3, 13, …
## $ distance_traveled    &lt;dbl&gt; 226, 370, 725, 266, 938, 148,…
## $ materials_gathered   &lt;dbl&gt; 0, 0, 0, 358, 305, 0, 1286, 1…
## $ materials_used       &lt;dbl&gt; 0, 38, 0, 61, 234, 170, 195, …
## $ damage_taken         &lt;dbl&gt; 282, 203, 206, 262, 437, 151,…
## $ damage_to_players    &lt;dbl&gt; 372, 354, 206, 286, 823, 122,…
## $ damage_to_structures &lt;dbl&gt; 538, 1403, 260, 3841, 1470, 4…
## $ won                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…
## $ player               &lt;int&gt; -5, -5, -5, -5, -5, -5, -5, -…
## $ gameId               &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10…
## $ startTime            &lt;dttm&gt; 2020-04-10 16:46:06, 2020-04…
## $ sessionId            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ lagSess              &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ delta                &lt;dbl&gt; 928.2763, 212.1983, 529.5711,…
## $ startTime2           &lt;dttm&gt; NA, 2020-04-10 17:05:06, 202…
## $ gameIdSession        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10…
## $ gameIdSession2       &lt;dbl&gt; 1, 4, 9, 16, 25, 36, 49, 64, …
```

---

# The Data

- Start with the basics:

--

  1. What is the unit of analysis?
  
  2. Which variables are we interested in?


---

# Prediction

`$$Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + \dots + \varepsilon$$`

--

- `\(Y\)`: victory (`won`)

--

- `\(X\)`: ??

--

  - In prediction, we don't care about **theory** or **research questions**

  - Just want to maximize **accuracy**...which `\(X\)`'s are the "best"?
  
  - But theory can still help us make sensible choices about which `\(X\)`'s to use
  
--

- Look at univariate &amp; conditional relationships

---

# The Data

- Outcome `\(Y\)`: `won`


``` r
require(scales)
fn %&gt;%
  summarise(`Win %` = percent(mean(won)))
```

```
## # A tibble: 1 × 1
##   `Win %`
##   &lt;chr&gt;  
## 1 30%
```

--

- Multivariate analysis?

---

# Which `\(X\)`?


``` r
fn %&gt;%
  group_by(mental_state) %&gt;%
  summarise(pr_win = mean(won))
```

```
## # A tibble: 2 × 2
##   mental_state pr_win
##   &lt;chr&gt;         &lt;dbl&gt;
## 1 high          0.234
## 2 sober         0.370
```

---

# Which `\(X\)`?


``` r
fn %&gt;%
  group_by(gameIdSession) %&gt;%
  summarise(pr_win = mean(won))
```

```
## # A tibble: 44 × 2
##    gameIdSession pr_win
##            &lt;int&gt;  &lt;dbl&gt;
##  1             1 0.0588
##  2             2 0.0588
##  3             3 0.206 
##  4             4 0.147 
##  5             5 0.147 
##  6             6 0.0588
##  7             7 0.206 
##  8             8 0.265 
##  9             9 0.412 
## 10            10 0.618 
## # ℹ 34 more rows
```

---

# Which `\(X\)`?


``` r
fn %&gt;%
  group_by(gameIdSession) %&gt;%
  summarise(pr_win = mean(won)) %&gt;%
  ggplot(aes(x = gameIdSession,y = pr_win)) + 
  geom_point()
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-43-1.png" style="display: block; margin: auto;" /&gt;

---

# Which `\(X\)`?


``` r
fn %&gt;%
  ggplot(aes(x = hits,y = won)) + 
  geom_point()
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-44-1.png" style="display: block; margin: auto;" /&gt;

---

# Which `\(X\)`?


``` r
fn %&gt;%
  ggplot(aes(x = hits,y = won)) + 
  geom_jitter()
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-45-1.png" style="display: block; margin: auto;" /&gt;

---

# Heatmaps

- Look at 3-dimensions of data

--

  - Done this before by tweaking `fill`, `color`, or `size`
  
--

- `geom_tile()`: create a heatmap


``` r
p &lt;- fn %&gt;%
  mutate(accuracy_decile = ntile(hits,n=10)) %&gt;% # Bin hits by decile (10%)
  group_by(accuracy_decile,mental_state) %&gt;% # Calculate average winning by mental state and accuracy
  summarise(pr_win = mean(won),
            .groups = 'drop') %&gt;%
  ggplot(aes(x = factor(mental_state),
             y = factor(accuracy_decile), # Both x and y-axes are factors
             fill = pr_win)) + # Fill by third dimension
  geom_tile() + # Creates rectangles
  scale_fill_gradient(limits = c(0,1)) # Set fill color (can do much more here)
```

---

# Heatmaps


``` r
p
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-47-1.png" style="display: block; margin: auto;" /&gt;

---

# Simplest Predictions

- Remember: regression is just fancier conditional means


``` r
fn &lt;- fn %&gt;%
  mutate(hits_decile = ntile(hits,n=10)) %&gt;% # Bin hits by decile (10%)
  group_by(hits_decile,mental_state) %&gt;% # Calculate average winning by mental state and accuracy
  mutate(prob_win = mean(won)) %&gt;% # use mutate() instead of summarise() to avoid collapsing the data
  mutate(pred_win = ifelse(prob_win &gt; .5,1,0)) %&gt;% # If the probability is greater than 50-50, predict a win
  ungroup()
```

---

# Simplest Predictions

- Conditional means


``` r
fn %&gt;%
  group_by(won,pred_win) %&gt;%
  summarise(nGames=n(),.groups = 'drop')
```

```
## # A tibble: 4 × 3
##     won pred_win nGames
##   &lt;dbl&gt;    &lt;dbl&gt;  &lt;int&gt;
## 1     0        0    625
## 2     0        1     41
## 3     1        0    241
## 4     1        1     50
```

--

- How good is this? Think about the underlying goal...we want a model that accurately predicts whether a game is won or not

- The `won` column is the **truth**...it tells us whether the game was won or not

- The `pred_win` column is our **prediction**

---

# Accuracy

- What is "accuracy"?

--

  - Proportion "correct" predictions
  
--

- For a binary outcome, "accuracy" has two dimensions

--

  - Proportion of correct `1`s: **Sensitivity**
  
  - Proportion of correct `0`s: **Specificity**
  
---

# Accuracy


``` r
(sumTab &lt;- fn %&gt;%
  group_by(won) %&gt;%
  mutate(total_games = n()) %&gt;%
  group_by(won,pred_win,total_games) %&gt;%
  summarise(nGames=n(),.groups = 'drop') %&gt;%
  mutate(prop = nGames / total_games))
```

```
## # A tibble: 4 × 5
##     won pred_win total_games nGames   prop
##   &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt;
## 1     0        0         666    625 0.938 
## 2     0        1         666     41 0.0616
## 3     1        0         291    241 0.828 
## 4     1        1         291     50 0.172
```

--

- Overall accuracy: (625+50) / (666+291) = 71%

- But we are doing **great** at predicting losses (94%)...

- ...and **terribly** at predicting wins (17%)

---

# Regression


``` r
fn %&gt;%
  ggplot(aes(x = damage_to_players,y = won)) + 
  geom_point() + 
  geom_smooth(method = 'lm')
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-51-1.png" style="display: block; margin: auto;" /&gt;

---

# Regression

- Binary outcome variable!

--

  - A linear regression is not the best solution
  
--

  - Predictions can exceed support of `\(Y\)`

--

- But it can still work! **linear probability model**


``` r
mLM &lt;- lm(won ~ hits + accuracy + mental_state,fn)
```

---

# Linear Regression


``` r
require(broom) # broom package makes it easy to read regression output
tidy(mLM) %&gt;% # This would be the same as summary(mLM)
  mutate_at(vars(-term),function(x) round(x,5))
```

```
## # A tibble: 4 × 5
##   term              estimate std.error statistic p.value
##   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)        0.219     0.0336       6.52       0
## 2 hits               0.00646   0.00065      9.91       0
## 3 accuracy          -0.725     0.108       -6.72       0
## 4 mental_statesober  0.155     0.0281       5.53       0
```


---

# Evaluating Predictions


``` r
mLM &lt;- lm(won ~ hits + accuracy + mental_state + damage_taken + head_shots + gameIdSession,fn)
fn %&gt;%
  mutate(preds = predict(mLM)) %&gt;%
  mutate(predBinary = ifelse(preds &gt; .5,1,0)) %&gt;%
  select(won,predBinary,preds)
```

```
## # A tibble: 957 × 3
##      won predBinary  preds
##    &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;
##  1     0          0 0.320 
##  2     0          0 0.239 
##  3     0          0 0.193 
##  4     0          0 0.285 
##  5     0          0 0.148 
##  6     0          0 0.175 
##  7     0          0 0.258 
##  8     0          0 0.115 
##  9     0          0 0.239 
## 10     1          0 0.0982
## # ℹ 947 more rows
```

---

# Evaluating Predictions



``` r
(sumTab &lt;- fn %&gt;%
  mutate(pred_win = ifelse(predict(mLM) &gt; .5,1,0)) %&gt;%
  group_by(won) %&gt;%
  mutate(total_games = n()) %&gt;%
  group_by(won,pred_win,total_games) %&gt;%
  summarise(nGames=n(),.groups = 'drop') %&gt;%
  mutate(prop = percent(nGames / total_games)) %&gt;%
  ungroup() %&gt;%
  mutate(accuracy = percent(sum((won == pred_win)*nGames) / sum(nGames))))
```

```
## # A tibble: 4 × 6
##     won pred_win total_games nGames prop  accuracy
##   &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt;  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   
## 1     0        0         666    615 92%   71%     
## 2     0        1         666     51 8%    71%     
## 3     1        0         291    226 78%   71%     
## 4     1        1         291     65 22%   71%
```

---

# Evaluating Predictions

- Overall accuracy is just the number of correct predictions (either `0` or `1`) out of all possible

--

  - Is 71% good?

--

  - What would the dumbest guess be? Never win! 70%

--

- Might also want to care about just `1`s

--

  - **Sensitivity**: Predicted wins / actual wins = 22%
  
--

- Also might care about just `0`s

--

  - **Specificity**: Predicted losses / actual losses = 92%
  
---

# Thresholds

- Shifting the threshold for `0` or `1` prediction can matter

--


``` r
fn %&gt;%
  mutate(pred_win = ifelse(predict(mLM) &gt; .4,1,0)) %&gt;%
  group_by(won) %&gt;%
  mutate(total_games = n()) %&gt;%
  group_by(won,pred_win,total_games) %&gt;%
  summarise(nGames=n(),.groups = 'drop') %&gt;%
  mutate(prop = percent(nGames / total_games)) %&gt;%
  ungroup() %&gt;%
  mutate(accuracy = percent(sum((won == pred_win)*nGames) / sum(nGames)))
```

```
## # A tibble: 4 × 6
##     won pred_win total_games nGames prop  accuracy
##   &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt;  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   
## 1     0        0         666    542 81.4% 72%     
## 2     0        1         666    124 18.6% 72%     
## 3     1        0         291    144 49.5% 72%     
## 4     1        1         291    147 50.5% 72%
```

---

# Thresholds

- Shifting the threshold for `0` or `1` prediction can matter


``` r
fn %&gt;%
  mutate(pred_win = ifelse(predict(mLM) &gt; .7,1,0)) %&gt;%
  group_by(won) %&gt;%
  mutate(total_games = n()) %&gt;%
  group_by(won,pred_win,total_games) %&gt;%
  summarise(nGames=n(),.groups = 'drop') %&gt;%
  mutate(prop = percent(nGames / total_games)) %&gt;%
  ungroup() %&gt;%
  mutate(accuracy = percent(sum((won == pred_win)*nGames) / sum(nGames)))
```

```
## # A tibble: 4 × 6
##     won pred_win total_games nGames prop  accuracy
##   &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt;  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   
## 1     0        0         666    663 99.5% 70%     
## 2     0        1         666      3 0.5%  70%     
## 3     1        0         291    280 96.2% 70%     
## 4     1        1         291     11 3.8%  70%
```

- Restricting to above 70% means we don't think anyone wins!

---

# Thresholds

- We could keep trying different values until we hit on one that maximizes our accuracy

--

- But this is inefficient! Let's loop it instead!

--



``` r
toplot &lt;- NULL
for(thresh in seq(0,1,by = .025)) {
  toplot &lt;- fn %&gt;%
  mutate(pred_win = ifelse(predict(mLM) &gt; thresh,1,0)) %&gt;%
  group_by(won) %&gt;%
  mutate(total_games = n()) %&gt;%
  group_by(won,pred_win,total_games) %&gt;%
  summarise(nGames=n(),.groups = 'drop') %&gt;%
  mutate(prop = nGames / total_games) %&gt;%
  ungroup() %&gt;%
  mutate(accuracy = sum((won == pred_win)*nGames) / sum(nGames)) %&gt;%
  mutate(threshold = thresh) %&gt;%
    bind_rows(toplot)
}
```

---

# Thresholds

- We might only care about accuracy by itself (although this is a bit naive)

.small[

``` r
toplot %&gt;%
  select(accuracy,threshold) %&gt;%
  distinct() %&gt;%
  ggplot(aes(x = threshold,y = accuracy)) + 
  geom_line()
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-59-1.png" style="display: block; margin: auto;" /&gt;
]

---

# Thresholds

.small[

``` r
toplot %&gt;%
  mutate(metric = ifelse(won == 1 &amp; pred_win == 1,'Sensitivity',
                         ifelse(won == 0 &amp; pred_win == 0,'Specificity',NA))) %&gt;%
  drop_na(metric) %&gt;%
  ggplot(aes(x = threshold,y = prop,color = metric)) + 
  geom_line()
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-60-1.png" style="display: block; margin: auto;" /&gt;
]


---

# ROC Curve

- Receiver-Operator Characteristic (ROC) Curve

--

- Commonly used to evaluate classification methods

--

  - X-axis: 1-specificity

  - Y-axis: sensitivity

--


``` r
p &lt;- toplot %&gt;%
  mutate(metric = ifelse(won == 1 &amp; pred_win == 1,'Sensitivity',
                         ifelse(won == 0 &amp; pred_win == 0,'Specificity',NA))) %&gt;%
  drop_na(metric) %&gt;%
  select(prop,metric,threshold) %&gt;%
  spread(metric,prop) %&gt;%
  arrange(desc(Specificity),Sensitivity) %&gt;%
  ggplot(aes(x = 1-Specificity,y = Sensitivity)) + 
  geom_line() + 
  xlim(c(0,1)) + ylim(c(0,1)) + 
  geom_abline(slope = 1,intercept = 0,linetype = 'dotted') + 
  ggridges::theme_ridges()
```

---

# ROC Curve


``` r
p
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-62-1.png" style="display: block; margin: auto;" /&gt;

--

- Better models have high levels of sensitivity **and** specificity at every threshold

---

# AUC Measure

- Area Under the Curve (AUC)

--

  - A single number summarizing classification performance
  
--


``` r
require(tidymodels)
roc_auc(data = fn %&gt;%
  mutate(pred_win = predict(mLM),
         truth = factor(won,levels = c('1','0'))) %&gt;%
  select(truth,pred_win),truth,pred_win)
```

```
## # A tibble: 1 × 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 roc_auc binary         0.736
```

---

# AUC

- What is a "good" AUC?

--

  - We know it is bounded between 0 (i.e., it predicts everything **perfectly wrong**) and 1 (i.e., it predicts everything **perfectly correct**)
  
  - But typically we don't see AUC values less than 0.5 (why is this?)
  
--

- AUC can be interpreted like numeric grades at Villanova (and for this class)

  - 0.95+ is amazing
  
  - 0.9 - 0.95 is very good
  
  - 0.8-range is B-tier
  
  - 0.7-range is C-tier
  
  - 0.6-range is really bad
  
  - AUC values less than 0.6 are failing

---

# Party time!

- Adding more variables / trying different combinations

--

- **Workflow**

--

  1. Train models
  
  2. Predict models
  
  3. Evaluate models
  
---

# Train models


``` r
m1 &lt;- lm(won ~ hits,fn)
m2 &lt;- lm(won ~ hits + head_shots,fn)
m3 &lt;- lm(won ~ hits + accuracy + head_shots,fn)
m4 &lt;- lm(won ~ hits + accuracy + head_shots + mental_state,fn)
m5 &lt;- lm(won ~ hits + accuracy + head_shots + mental_state + distance_traveled,fn)
m6 &lt;- lm(won ~ hits + accuracy + mental_state + head_shots + distance_traveled + gameIdSession,fn)
```

---

# Predict models


``` r
toEval &lt;- fn %&gt;%
  mutate(m1Preds = predict(m1),
         m2Preds = predict(m2),
         m3Preds = predict(m3),
         m4Preds = predict(m4),
         m5Preds = predict(m5),
         m6Preds = predict(m6),
         truth = factor(won,levels = c('1','0')))
```

---

# Evaluate models


``` r
rocRes &lt;- NULL
for(model in 1:6) {
  rocRes &lt;- roc_auc(toEval,truth,paste0('m',model,'Preds')) %&gt;%
    mutate(model = paste0('Model ',model)) %&gt;%
    bind_rows(rocRes)
}
```

---

# Evaluate models


``` r
rocRes %&gt;%
  ggplot(aes(x = .estimate,y = reorder(model,.estimate))) + 
  geom_bar(stat = 'identity') + 
  ggridges::theme_ridges() + labs(x = 'AUC',y = 'Regression Model')
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-67-1.png" style="display: block; margin: auto;" /&gt;

---

# OVERFITTING

- Cross validation to the rescue!

.tiny[

``` r
set.seed(123)
cvRes &lt;- NULL
for(i in 1:100) {
  # Cross validation prep
  inds &lt;- sample(1:nrow(fn),size = round(nrow(fn)*.8),replace = F)
  train &lt;- fn %&gt;% slice(inds)
  test &lt;- fn %&gt;% slice(-inds)

  # Training models
  m1 &lt;- lm(won ~ hits,train)
  m2 &lt;- lm(won ~ hits + head_shots,train)
  m3 &lt;- lm(won ~ hits + accuracy + head_shots,train)
  m4 &lt;- lm(won ~ hits + accuracy + head_shots + mental_state,train)
  m5 &lt;- lm(won ~ hits + accuracy + head_shots + mental_state + distance_traveled,train)
  m6 &lt;- lm(won ~ hits + accuracy + mental_state + head_shots + distance_traveled + gameIdSession,train)

  # Predicting models
  toEval &lt;- test %&gt;%
    mutate(m1Preds = predict(m1,newdata = test),
           m2Preds = predict(m2,newdata = test),
           m3Preds = predict(m3,newdata = test),
           m4Preds = predict(m4,newdata = test),
           m5Preds = predict(m5,newdata = test),
           m6Preds = predict(m6,newdata = test),
           truth = factor(won,levels = c('1','0')))

  # Evaluating models
  rocResBS &lt;- NULL
  for(model in 1:6) {
    rocResBS &lt;- roc_auc(toEval,truth,paste0('m',model,'Preds')) %&gt;%
      mutate(model = as.character(get(paste0('m',model))$call$formula)[3]) %&gt;%
      bind_rows(rocResBS)
  }
  cvRes &lt;- rocResBS %&gt;%
    mutate(bsInd = i) %&gt;%
    bind_rows(cvRes)
}
```
]
---

# Cross Validation AUC


``` r
cvRes %&gt;%
  ggplot(aes(x = .estimate,y = factor(reorder(model,.estimate)))) + 
  geom_boxplot() + labs(x = 'Distribution of AUC',y = 'Specification')
```

&lt;img src="psc7000_lecture_6_slides_files/figure-html/unnamed-chunk-69-1.png" style="display: block; margin: auto;" /&gt;

---

# Conclusion

- Classification is just a type of prediction

--

  - We used linear regression
  
--

  - But there are **much** fancier algorithms out there
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": "%current%/%total%",
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
