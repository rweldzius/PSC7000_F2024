---
title: "Problem Set 3"
subtitle: "Multivariate Visualization"
author: "[YOUR NAME]"
institute: "Villanova University (PSC7000)"
date: "Due Date: 2024-09-27"
output:
  html_document: default
---

```{r,include=F}
knitr::opts_chunk$set(error=TRUE)
```


## Getting Set Up

Open `RStudio` and create a new RMarkDown file (`.Rmd`) by going to `File -> New File -> R Markdown...`.
Accept defaults and save this file as `[LAST NAME]_ps5.Rmd` to your `code` folder.

Copy and paste the contents of this `.Rmd` file into your `[LAST NAME]_ps5.Rmd` file. Then change the `author: [Your Name]` to your name.

We will be using two datasets this week. The first is the `Pres2020_PV.Rds` file from the course [github page](https://github.com/rweldzius/PSC7000_F2024/raw/main/Data/Pres2020_PV.Rds) and should be saved to an object called `pres`. The second is the `Pres2020_StatePolls.Rds` file from the course [github page](https://github.com/rweldzius/PSC7000_F2024/raw/main/Data/Pres2020_StatePolls.Rds) and should be saved to an object called `state`.

All of the following questions should be answered in this `.Rmd` file. There are code chunks with incomplete code that need to be filled in. To submit, compile (i.e., `knit`) the completed problem set and upload the PDF file to Brightspace on Friday by midnight. Instructions for how to compile the output as a PDF can be found in [Problem Set 0](https://github.com/rweldzius/PSC7000_F2024/blob/main/Psets/psc7000_pset_0.pdf) and in this [gif tutorial](https://github.com/rweldzius/PSC7000_F2024//blob/main/Psets/save_as_pdf.gif). 

This problem set is worth 5 total points, plus 1 extra credit point. The point values for each question are indicated in brackets below. To receive full credit, you must have the correct code. In addition, some questions ask you to provide a written response in addition to the code.

You will be deducted 1 point for each day late the problem set is submitted, and 1 point for failing to submit in the correct format.

You are free to rely on whatever resources you need to complete this problem set, including lecture notes, lecture presentations, Google, your classmates...you name it. However, the final submission must be complete by you. There are no group assignments.

*Note that the professor will not respond to Campuswire posts after 5PM on Friday, so don't wait until the last minute to get started!*

**Good luck!**



## Question 0
*Require `tidyverse` and load the `Pres2020_PV.Rds` data to an object called `pres`.*

```{r}
require(tidyverse)
pres <- read_rds("https://github.com/rweldzius/PSC7000_F2024/raw/main/Data/Pres2020_PV.Rds")
```

## Question 1 [1 point]
*Consider the following hypothesis: "Most Americans don't pay very much attention to politics, and don't know who they will vote for until very close to the election. Therefore polling predictions should be more accurate closer to the election." Based on this hypothesis and theoretical intuition, which variable is the $X$ variable and which is the $Y$ variable(s)?*

>- The $X$ variable is time to election; the $Y$ variable is the accuracy of the polling. 

*Now let's first look at each variable by itself using univariate visualization. First, plot the total number of polls per start date in the data. NB: you will have convert `StartDate` to a `date` class with `as.Date()`. If you need help, see [this post](https://www.r-bloggers.com/2013/08/date-formats-in-r/). Do you observe a pattern in the number of polls over time? Why do you think this is?*
```{r}
pres %>%
  mutate(StartDate = as.Date(StartDate,'%m/%d/%Y')) %>% # Convert to date
  ggplot(aes(x = StartDate)) + # Visualize the variable using univariate principles
  geom_bar(stat='count') + # Choose the correct `geom`
  labs(title = 'Total Polls Started by Day',
       x = 'Start Date',
       y = 'Number of Polls') # Make sure it is clearly labeled
```

>- There are more polls fielded the closer we get to the election.


## Question 2 [1 point]
*Next, let's look at the other variables. Calculate the* **prediction error** *for Biden (call this variable `demErr`) and Trump (call this variable `repErr`) such that positive values mean that the poll* **overestimated** *the candidate's popular vote share (`DemCertVote` for Biden and `RepCertVote` for Trump).*

```{r}
pres <- pres %>%
  mutate(demErr = Biden - DemCertVote,
         repErr = Trump - RepCertVote) 
```


*Plot the Biden and Trump prediction errors on a single plot using `geom_bar()`, with red indicating Trump and blue indicating Biden (make sure to set alpha to some value less than 1 to increase the transparency!). Add vertical lines for the average prediction error for both candidates (colored appropriately) as well as a vertical line indicating no prediction error.*


```{r}
pres %>%
  ggplot() +
  geom_bar(aes(x=demErr), fill="blue",alpha=.5)+ 
  geom_bar(aes(x=repErr), fill="red",alpha=.5) +
  labs(title='Poll Mistakes by Biden (blue) and Trump (red)',
       x = 'Prediction Error: Prediction - True Support',
       y = 'Number of Polls') + 
  theme_bw() + # this creates a white background with grey grid marks and a black border around the figure
  geom_vline(xintercept = 0, lwd = 2, color="black") + # lwd defines the width of the line
  geom_vline(xintercept = mean(pres$demErr, na.rm=T),color='blue',linetype='dashed') + 
  geom_vline(xintercept= mean(pres$repErr, na.rm=T),color='red',linetype='dashed')
```

*Do you observe a systematic bias toward one candidate or the other?*

>- I observe a systematic bias against both candidates where the polls underestimate the amount of support for Biden and Trump. However, the magnitude of this bias against Trump is larger than the bias against Biden.

## Question 3 [1 point]
*Plot the average prediction error for Trump (red) and Biden (blue) by start date using `geom_point()` and add two curvey lines of best fit using `geom_smooth()`. Make sure that the curvey line for Trump is also red, and the curvey line for Biden is also blue!*


```{r}
pres %>% 
  mutate(StartDate = as.Date(StartDate,'%m/%d/%Y')) %>% 
  group_by(StartDate) %>%
  summarise(demErr = mean(demErr), repErr = mean(repErr)) %>%
  ggplot() + 
  geom_point(aes(x = StartDate,y = demErr),color = 'blue') + 
  geom_point(aes(x = StartDate,y = repErr),color = 'red') + 
  geom_smooth(aes(x = StartDate,y = demErr),color = 'blue') + 
  geom_smooth(aes(x = StartDate,y = repErr),color = 'red') + 
  labs(title = "Prediction Errors for Trump (red) and Biden (blue) by Date", 
       x = "Start Date", 
       y = "Average Prediction Error") + 
  geom_vline(xintercept = 0,linetype = 'dashed') + 
  geom_hline(yintercept = 0,linetype = 'dashed') + 
  theme_bw() + 
  scale_y_continuous(limits = c(min(c(pres$demErr,pres$repErr),na.rm=T), max(c(pres$demErr,pres$repErr),na.rm=T)))
```

*What pattern do you observe over time, if any? Does this support the hypothesis presented in Question 1 above?*

>- I observe a gradual decline in the prediction error over time, where polls underestimate both Trump and Biden less and less.

## Question 4 [1 point]
*Can we do better by aggregating state-level polls? Load the [Pres2020_StatePolls.Rds] to an object called `state`. First, create two new variables `demErr` and `repErr` just as you did in Question 2. Then recreate the same overtime plot comparing Biden and Trump prediction errors as you did in Question 3. What do you observe?*

```{r}
state <- read_rds('https://github.com/rweldzius/PSC4175_F2024/raw/main/Data/Pres2020_StatePolls.Rds') # Read in the data

state <- state %>%
   mutate(demErr = Biden - BidenCertVote,
          repErr = Trump - TrumpCertVote)

state %>%
  mutate(StartDate = as.Date(StartDate,'%m/%d/%Y')) %>% 
  group_by(StartDate) %>%
  summarise(MeanDemErr = mean(demErr,na.rm=T),
            MeanRepErr = mean(repErr,na.rm=T)) %>%
  ungroup() %>%
  ggplot() + 
  geom_point(aes(x = StartDate,y = MeanDemErr),color = 'blue') + 
  geom_point(aes(x = StartDate,y = MeanRepErr),color = 'red') + 
  geom_smooth(aes(x = StartDate,y = MeanDemErr),color = 'blue') + 
  geom_smooth(aes(x = StartDate,y = MeanRepErr),color = 'red') + 
  labs(title = "Prediction Errors for Trump (red) and Biden (blue) by Date & State", 
       x = "Start Date", 
       y = "Average Prediction Error") + 
  geom_hline(yintercept = 0,linetype = 'dashed') + 
  theme_bw()
```

>- I observe that Biden's prediction error is declines in late Spring/early Summer and remains close to the true vote count after July, whereas Trump's prediction error remains greater than Biden's but declines after July. 

## Question 5 [1 point]
*One other explanation for inaccurate state polls is that some states do not have many polls run. Calculate the anti-Trump/pro-Biden bias for each state by subtracting the `repErr` from the `demErr` (call this new variable `bidenBias`). Then calculate the average bias by state AND calculate the number of polls in that state. Finally, plot the relationship between the number of polls and the extent of bias. Does the data support the theory that states with more polls were predicted more accurately?*

```{r}
state <- state %>%
  mutate(bidenBias = demErr - repErr) 


state %>%
  group_by(StateName) %>%
  summarise(bidenBias = mean(bidenBias,na.rm=T),  
            noPolls = n()) %>% 
  ungroup() %>%
  ggplot(aes(x = noPolls,     
             y = bidenBias)) +   
  geom_point(shape=1,size=2) +
  geom_smooth(method='lm',color='black') + 
  labs(title = 'Biden Bias by Number of Polls, by State',
       x = 'Number of Polls',
       y = 'Biden Bias') +
  theme_bw()
```

>- There is suggestive evidence that states with more polls were predicted more accurately. Extra: However, the relationship is not statistically significant as seen by the width of the confidence band around the line of best fit. 

## Extra Credit [1 point]
*Do polls that underestimate Trump's support overestimate Biden's support? Investigate this question using both the national data (`pres`) and the state data (`state`). Use a scatterplot to test, combined with a (straight) line of best fit. Then, calculate the proportion of polls that (1) underestimate both Trump and Biden, (2) underestimate Trump and overestimate Biden, (3) overestimate Trump and underestimate Biden, (4) overestimate both candidates. In these analyses, define "overestimate" as prediction errors greater than or equal to zero, whereas "underestimate" should be prediction errors less than zero. What do you conclude? Is there any evidence of an anti-Trump bias in national polling? What about state polling?*
```{r}
# National scatterplot
pres %>% 
  ggplot(aes(x = demErr,y = repErr)) + 
  geom_point(size = 3,alpha = .3) + 
  geom_smooth(method = 'lm') + 
  labs(title = "Prediction Errors for Trump and Biden", 
       x = "Biden's Prediction Error", 
       y = "Trump's Prediction Error") +
  geom_vline(xintercept = 0, linetype = 'dashed') + 
  geom_hline(yintercept = 0,linetype = 'dashed') + 
  theme_bw()


# Alternative #1
pres %>% 
  count(demErr,repErr) %>%
  ggplot(aes(x = demErr,y = repErr, size = n)) + 
  geom_point() + 
  geom_smooth(method = 'lm') + 
  labs(title = "Prediction Errors for Trump and Biden", 
       x = "Biden's Prediction Error", 
       y = "Trump's Prediction Error") +
  geom_vline(xintercept = 0, linetype = 'dashed') + 
  geom_hline(yintercept = 0,linetype = 'dashed') + 
  theme_bw()

# Alternative #2
pres %>% 
  ggplot(aes(x = demErr,y = repErr)) + 
  geom_jitter() + 
  geom_smooth(method = 'lm') + 
  labs(title = "Prediction Errors for Trump and Biden", 
       x = "Biden's Prediction Error", 
       y = "Trump's Prediction Error") +
  geom_vline(xintercept = 0, linetype = 'dashed') + 
  geom_hline(yintercept = 0,linetype = 'dashed') + 
  theme_bw()


# National proportions: 4 different types of polls
pres %>% 
  summarise(UNboth = mean(demErr < 0 & repErr < 0), 
            UNTrOVBi = mean(demErr >= 0 & repErr < 0), 
            OVTrUNBi = mean(demErr < 0 & repErr >= 0), 
            OVboth = mean(demErr >= 0 & repErr >= 0))

# State scatterplot
state %>% 
  ggplot(aes(x = demErr,y = repErr)) + 
  geom_point(size = 3,alpha = .3) + 
  geom_smooth(method = 'lm') + 
  labs(title = "Prediction Errors for Trump and Biden", 
       x = "Biden's Prediction Error", 
       y = "Trump's Prediction Error") +
  geom_vline(xintercept = 0, linetype = 'dashed') + 
  geom_hline(yintercept = 0,linetype = 'dashed') + 
  theme_bw()

# State proportions: 4 different types of polls
state %>% 
  summarise(UNboth = mean(demErr < 0 & repErr < 0), 
            UNTrOVBi = mean(demErr >= 0 & repErr < 0), 
            OVTrUNBi = mean(demErr < 0 & repErr >= 0), 
            OVboth = mean(demErr >= 0 & repErr >= 0))
```

>- The results from the national data show that the polls which underestimate Trump's support also underestimated Biden's support, as indicated by the positive slope. Almost 60% of the polls underpredicted both Biden and Trump, whereas 0% of the polls overpredicted both. Nevertheless, there is still some evidence of an anti-Trump bias, since 39% of polls underpredicted Trump and overpredicted Biden, while only 1.7% of polls overpredicted Trump and underpredicted Biden. In the state data, the evidence is even stronger. Here we find a negative relationship, meaning that polls which overestimated Biden's support ALSO underestimated Trump's support. Furthermore, 59% of polls overpredicted Biden and underpredicted Trump, compared to only 5% of polls which overpredicted Trump and underpredicted Biden. In summary, I would conclude there is stronger evidence of an anti-Trump bias in the state polls, compared to the national polls. [RUBRIC: 0.75 points for not using `alpha` or `size` or `geom_jitter` to account for multiple polls on the same points. 0.75 points for incorrectly calculating the four types of polls for both the state and national-level data. 0.5 points for missing labels. 0.25 points if written response did not acknowledge that the patterns found in the national data do not appear in the state-level data (i.e., the line of best fit is positive in the national data, but negative in the state data). 0 points if no attempt was made.]
